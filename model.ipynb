{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "Setup complete ‚úÖ (4 CPUs, 8.0 GB RAM, 107.3/112.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Found https://ultralytics.com/images/zidane.jpg locally at zidane.jpg\n",
      "image 1/1 /Users/macbookair/Documents/Uni/PAAI/Code/zidane.jpg: 384x640 2 persons, 1 tie, 453.9ms\n",
      "Speed: 66.7ms preprocess, 453.9ms inference, 30.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Run inference on an image with YOLOv8n\n",
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 21.4M/780M [04:43<2:47:59, 79.0kB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mdownload_url_to_file(\u001b[39m'\u001b[39;49m\u001b[39mhttps://ultralytics.com/assets/coco2017val.zip\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtmp.zip\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# download (780M - 5000 images)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39munzip -q tmp.zip -d datasets && rm tmp.zip  # unzip\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/hub.py:651\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_size, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m progress,\n\u001b[1;32m    649\u001b[0m           unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m    650\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m         buffer \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[1;32m    652\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    653\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:462\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 462\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:506\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    503\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    508\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
    "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Dataset 'coco8.yaml' images not found ‚ö†Ô∏è, missing path '/Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/images/val'\n",
      "Downloading https://ultralytics.com/assets/coco8.zip to '/Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8.zip'...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433k/433k [00:00<00:00, 682kB/s]\n",
      "Unzipping /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8.zip to /Users\n",
      "Dataset download success ‚úÖ (6.3s), saved to \u001b[1m/Users/macbookair/Documents/Uni/PAAI/Code/datasets\u001b[0m\n",
      "\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/labels/va\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/labels/val.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.621      0.833      0.888       0.63\n",
      "                person          4         10      0.721        0.5      0.519      0.269\n",
      "                   dog          4          1       0.37          1      0.995      0.597\n",
      "                 horse          4          2      0.751          1      0.995      0.631\n",
      "              elephant          4          2      0.505        0.5      0.828      0.394\n",
      "              umbrella          4          1      0.564          1      0.995      0.995\n",
      "          potted plant          4          1      0.814          1      0.995      0.895\n",
      "Speed: 25.7ms preprocess, 1087.3ms inference, 0.0ms loss, 22.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "!yolo val model=yolov8n.pt data=coco8.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/labels/\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco8/labels/va\u001b[0m\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G     0.9793      3.465      1.356         32        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.902      0.518      0.727      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G      1.363      2.846      1.662         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.906      0.524      0.738      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.294      3.584      1.698         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.907      0.525      0.729       0.52\n",
      "\n",
      "3 epochs completed in 0.015 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.907      0.525      0.743      0.527\n",
      "                person          4         10       0.94        0.3      0.502      0.236\n",
      "                   dog          4          1          1          0      0.332      0.189\n",
      "                 horse          4          2          1      0.852      0.995      0.747\n",
      "              elephant          4          2          1          0      0.638        0.2\n",
      "              umbrella          4          1      0.759          1      0.995      0.895\n",
      "          potted plant          4          1      0.745          1      0.995      0.895\n",
      "Speed: 19.1ms preprocess, 379.7ms inference, 0.0ms loss, 6.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv8n on COCO8 for 3 epochs\n",
    "!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.1.0...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 6.7s, saved as 'yolov8n.torchscript' (12.4 MB)\n",
      "\n",
      "Export complete (11.1s)\n",
      "Results saved to \u001b[1m/Users/macbookair/Documents/Uni/PAAI/Code\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
     ]
    }
   ],
   "source": [
    "!yolo export model=yolov8n.pt format=torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Py Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=2, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/2         0G      1.096      1.365      1.202        201        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [02:21<00:00, 17.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:41<00:00, 10.27s/it]\n",
      "                   all        128        929      0.647      0.518      0.595      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/2         0G      1.217      1.447      1.268        136        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [02:55<00:00, 21.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:45<00:00, 11.49s/it]\n",
      "                   all        128        929      0.658      0.538      0.612      0.454\n",
      "\n",
      "2 epochs completed in 0.118 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:39<00:00,  9.85s/it]\n",
      "                   all        128        929       0.66      0.539      0.613      0.454\n",
      "                person        128        254      0.787      0.657       0.76      0.543\n",
      "               bicycle        128          6       0.61      0.333       0.32      0.276\n",
      "                   car        128         46      0.731      0.217      0.267      0.169\n",
      "            motorcycle        128          5      0.746          1      0.962      0.757\n",
      "              airplane        128          6      0.779      0.667      0.798       0.57\n",
      "                   bus        128          7      0.596      0.714      0.719      0.585\n",
      "                 train        128          3       0.53      0.667      0.789      0.681\n",
      "                 truck        128         12      0.906      0.333      0.463      0.284\n",
      "                  boat        128          6      0.305      0.167      0.369      0.239\n",
      "         traffic light        128         14      0.699      0.214      0.206       0.14\n",
      "             stop sign        128          2          1      0.922      0.995      0.701\n",
      "                 bench        128          9      0.771       0.38      0.616      0.379\n",
      "                  bird        128         16      0.916      0.682      0.898      0.537\n",
      "                   cat        128          4      0.861          1      0.995      0.772\n",
      "                   dog        128          9      0.659      0.778      0.807      0.594\n",
      "                 horse        128          2      0.582          1      0.995      0.597\n",
      "              elephant        128         17      0.903      0.824      0.872      0.704\n",
      "                  bear        128          1      0.652          1      0.995      0.895\n",
      "                 zebra        128          4       0.86          1      0.995      0.965\n",
      "               giraffe        128          9      0.657      0.889      0.943       0.69\n",
      "              backpack        128          6      0.586      0.333      0.451      0.315\n",
      "              umbrella        128         18      0.753      0.509      0.671      0.449\n",
      "               handbag        128         19      0.413     0.0435      0.159     0.0872\n",
      "                   tie        128          7      0.677      0.571      0.683       0.48\n",
      "              suitcase        128          4      0.565          1      0.845      0.551\n",
      "               frisbee        128          5       0.61        0.8      0.732       0.64\n",
      "                  skis        128          1      0.435          1      0.497      0.162\n",
      "             snowboard        128          7      0.895      0.714      0.768      0.486\n",
      "           sports ball        128          6      0.741        0.5      0.573      0.321\n",
      "                  kite        128         10      0.564        0.3      0.406      0.152\n",
      "          baseball bat        128          4      0.519        0.5      0.525      0.251\n",
      "        baseball glove        128          7      0.643      0.429      0.429      0.308\n",
      "            skateboard        128          5      0.788        0.6        0.6        0.4\n",
      "         tennis racket        128          7      0.616      0.286        0.4      0.278\n",
      "                bottle        128         18      0.434      0.333      0.387      0.225\n",
      "            wine glass        128         16      0.724      0.375      0.527      0.312\n",
      "                   cup        128         36      0.761      0.266      0.431      0.294\n",
      "                  fork        128          6          1      0.298      0.372       0.24\n",
      "                 knife        128         16      0.632      0.562      0.595      0.347\n",
      "                 spoon        128         22      0.701      0.182      0.351      0.184\n",
      "                  bowl        128         28      0.663      0.632      0.615      0.516\n",
      "                banana        128          1     0.0549      0.274      0.199     0.0491\n",
      "              sandwich        128          2      0.368      0.604      0.745      0.745\n",
      "                orange        128          4          1      0.404      0.995      0.666\n",
      "              broccoli        128         11      0.549      0.182      0.256      0.213\n",
      "                carrot        128         24      0.529      0.375      0.637      0.396\n",
      "               hot dog        128          2      0.552          1      0.828      0.795\n",
      "                 pizza        128          5      0.639          1      0.962       0.84\n",
      "                 donut        128         14      0.613          1      0.918      0.828\n",
      "                  cake        128          4      0.749          1      0.995      0.834\n",
      "                 chair        128         35      0.467      0.514      0.431      0.257\n",
      "                 couch        128          6      0.527      0.333      0.627      0.507\n",
      "          potted plant        128         14      0.501      0.501       0.66      0.454\n",
      "                   bed        128          3          1      0.595      0.863      0.646\n",
      "          dining table        128         13      0.579       0.53      0.514      0.415\n",
      "                toilet        128          2      0.642        0.5      0.828      0.796\n",
      "                    tv        128          2      0.556        0.5       0.62      0.596\n",
      "                laptop        128          3          1          0      0.333       0.26\n",
      "                 mouse        128          2          1          0          0          0\n",
      "                remote        128          8      0.751        0.5      0.568      0.476\n",
      "            cell phone        128          8          0          0     0.0415     0.0227\n",
      "             microwave        128          3      0.472          1      0.913      0.765\n",
      "                  oven        128          5       0.35        0.4      0.361      0.285\n",
      "                  sink        128          6       0.35      0.167      0.249      0.169\n",
      "          refrigerator        128          5      0.731      0.553      0.629      0.503\n",
      "                  book        128         29       0.55     0.0862      0.257      0.137\n",
      "                 clock        128          9       0.78      0.788      0.895      0.738\n",
      "                  vase        128          2      0.447          1      0.828      0.795\n",
      "              scissors        128          1          1          0      0.249     0.0541\n",
      "            teddy bear        128         21      0.831      0.333      0.595      0.407\n",
      "            toothbrush        128          5          1       0.46      0.761      0.488\n",
      "Speed: 7.4ms preprocess, 267.7ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:43<00:00,  5.41s/it]\n",
      "                   all        128        929      0.655       0.53      0.606      0.451\n",
      "                person        128        254      0.792      0.654      0.761      0.545\n",
      "               bicycle        128          6      0.614      0.333      0.318      0.275\n",
      "                   car        128         46      0.742      0.217      0.266      0.169\n",
      "            motorcycle        128          5       0.77          1      0.962      0.757\n",
      "              airplane        128          6      0.783      0.667      0.798       0.57\n",
      "                   bus        128          7      0.584      0.714      0.719      0.582\n",
      "                 train        128          3      0.531      0.667      0.789      0.681\n",
      "                 truck        128         12      0.874      0.333      0.444      0.245\n",
      "                  boat        128          6      0.384      0.167      0.364      0.203\n",
      "         traffic light        128         14      0.745      0.214      0.206       0.14\n",
      "             stop sign        128          2          1       0.92      0.995        0.7\n",
      "                 bench        128          9      0.769      0.376      0.616      0.379\n",
      "                  bird        128         16      0.916      0.681      0.899      0.539\n",
      "                   cat        128          4      0.863          1      0.995      0.773\n",
      "                   dog        128          9       0.66      0.778      0.807      0.594\n",
      "                 horse        128          2      0.585          1      0.995      0.597\n",
      "              elephant        128         17      0.905      0.824      0.872      0.704\n",
      "                  bear        128          1      0.654          1      0.995      0.895\n",
      "                 zebra        128          4       0.86          1      0.995      0.965\n",
      "               giraffe        128          9      0.804      0.915      0.951      0.765\n",
      "              backpack        128          6      0.572      0.333      0.434      0.291\n",
      "              umbrella        128         18      0.752      0.505       0.67      0.448\n",
      "               handbag        128         19        0.4     0.0421      0.141      0.075\n",
      "                   tie        128          7      0.681      0.571      0.683       0.48\n",
      "              suitcase        128          4      0.569          1      0.845      0.551\n",
      "               frisbee        128          5      0.614        0.8      0.732      0.639\n",
      "                  skis        128          1      0.441          1      0.497      0.162\n",
      "             snowboard        128          7      0.904      0.714      0.769      0.488\n",
      "           sports ball        128          6      0.744        0.5      0.556      0.314\n",
      "                  kite        128         10      0.567        0.3      0.408      0.154\n",
      "          baseball bat        128          4      0.595      0.392      0.445       0.21\n",
      "        baseball glove        128          7      0.645      0.429      0.429       0.33\n",
      "            skateboard        128          5      0.877        0.6        0.6       0.44\n",
      "         tennis racket        128          7      0.635      0.286      0.417      0.298\n",
      "                bottle        128         18      0.471      0.333       0.37      0.213\n",
      "            wine glass        128         16      0.733      0.375      0.508      0.311\n",
      "                   cup        128         36      0.741      0.278      0.429      0.295\n",
      "                  fork        128          6      0.588      0.167      0.283      0.206\n",
      "                 knife        128         16      0.629      0.562      0.584      0.348\n",
      "                 spoon        128         22      0.826      0.218      0.365      0.198\n",
      "                  bowl        128         28      0.696      0.654      0.657      0.535\n",
      "                banana        128          1          0          0      0.142     0.0433\n",
      "              sandwich        128          2      0.365      0.595      0.497      0.497\n",
      "                orange        128          4          1        0.4      0.995      0.666\n",
      "              broccoli        128         11      0.539      0.182      0.265      0.215\n",
      "                carrot        128         24      0.627      0.421      0.629      0.389\n",
      "               hot dog        128          2      0.543          1      0.828      0.828\n",
      "                 pizza        128          5      0.655          1      0.995      0.852\n",
      "                 donut        128         14      0.614          1      0.914       0.82\n",
      "                  cake        128          4      0.614          1      0.995      0.834\n",
      "                 chair        128         35      0.482      0.514      0.418      0.251\n",
      "                 couch        128          6      0.627        0.5      0.723      0.583\n",
      "          potted plant        128         14      0.505        0.5       0.66      0.454\n",
      "                   bed        128          3          1      0.588      0.863      0.681\n",
      "          dining table        128         13      0.519      0.538      0.507      0.398\n",
      "                toilet        128          2      0.645        0.5      0.828      0.796\n",
      "                    tv        128          2      0.559        0.5       0.62      0.596\n",
      "                laptop        128          3          1          0      0.311      0.251\n",
      "                 mouse        128          2          1          0          0          0\n",
      "                remote        128          8      0.756        0.5      0.599      0.494\n",
      "            cell phone        128          8          0          0     0.0426     0.0234\n",
      "             microwave        128          3       0.44      0.799       0.83      0.716\n",
      "                  oven        128          5      0.349        0.4      0.361      0.285\n",
      "                  sink        128          6      0.378      0.167      0.247       0.17\n",
      "          refrigerator        128          5       0.59        0.4      0.604       0.47\n",
      "                  book        128         29      0.404      0.069      0.272      0.144\n",
      "                 clock        128          9      0.779      0.785      0.898      0.738\n",
      "                  vase        128          2       0.45          1      0.828      0.795\n",
      "              scissors        128          1          1          0      0.249     0.0545\n",
      "            teddy bear        128         21      0.848      0.333      0.594      0.406\n",
      "            toothbrush        128          5      0.686      0.447      0.722      0.477\n",
      "Speed: 8.0ms preprocess, 290.0ms inference, 0.0ms loss, 6.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train32\u001b[0m\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /Users/macbookair/Documents/Uni/PAAI/Code/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 314.0ms\n",
      "Speed: 15.0ms preprocess, 314.0ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/train3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.1.0...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 6.0s, saved as 'runs/detect/train3/weights/best.torchscript' (12.5 MB)\n",
      "\n",
      "Export complete (8.3s)\n",
      "Results saved to \u001b[1m/Users/macbookair/Documents/Uni/PAAI/Code/runs/detect/train3/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs/detect/train3/weights/best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs/detect/train3/weights/best.torchscript imgsz=640 data=/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/cfg/datasets/coco128.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "results = model.train(data='coco128.yaml', epochs=2)  # train the model\n",
    "results = model.val()  # evaluate model performance on the validation set\n",
    "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
    "results = model.export(format='torchscript')  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task\n",
    "1. Detection\n",
    "2. Segmentation\n",
    "3. Classificatiom\n",
    "4. Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.206 available üòÉ Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# load a pretrained YOLOv8n detection model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcoco128.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model(\u001b[39m'\u001b[39m\u001b[39mhttps://ultralytics.com/images/bus.jpg\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# predict on an image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/engine/model.py:336\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    334\u001b[0m     args[\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path\n\u001b[0;32m--> 336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m (trainer \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_smart_load(\u001b[39m'\u001b[39;49m\u001b[39mtrainer\u001b[39;49m\u001b[39m'\u001b[39;49m))(overrides\u001b[39m=\u001b[39;49margs, _callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks)\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mget_model(weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, cfg\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39myaml)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/engine/trainer.py:85\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(cfg, overrides)\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_resume(overrides)\n\u001b[0;32m---> 85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m select_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdevice, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/utils/torch_utils.py:143\u001b[0m, in \u001b[0;36mselect_device\u001b[0;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[1;32m    141\u001b[0m     arg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# revert to CPU\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCPU (\u001b[39m\u001b[39m{\u001b[39;00mget_cpu_info()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    144\u001b[0m     arg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/utils/torch_utils.py:60\u001b[0m, in \u001b[0;36mget_cpu_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcpuinfo\u001b[39;00m  \u001b[39m# pip install py-cpuinfo\u001b[39;00m\n\u001b[1;32m     59\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbrand_raw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhardware_raw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39march_string_raw\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# info keys sorted by preference (not all keys always available)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m info \u001b[39m=\u001b[39m cpuinfo\u001b[39m.\u001b[39;49mget_cpu_info()  \u001b[39m# info dict\u001b[39;00m\n\u001b[1;32m     61\u001b[0m string \u001b[39m=\u001b[39m info\u001b[39m.\u001b[39mget(k[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m k[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m info \u001b[39melse\u001b[39;00m k[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m k[\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m info \u001b[39melse\u001b[39;00m k[\u001b[39m2\u001b[39m], \u001b[39m'\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m string\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m(R)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mCPU \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m@ \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/cpuinfo/cpuinfo.py:2759\u001b[0m, in \u001b[0;36mget_cpu_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m \u001b[39mReturns the CPU info by using the best sources of information for your OS.\u001b[39;00m\n\u001b[1;32m   2754\u001b[0m \u001b[39mReturns the result in a dict\u001b[39;00m\n\u001b[1;32m   2755\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m   2757\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m-> 2759\u001b[0m output \u001b[39m=\u001b[39m get_cpu_info_json()\n\u001b[1;32m   2761\u001b[0m \u001b[39m# Convert JSON to Python with non unicode strings\u001b[39;00m\n\u001b[1;32m   2762\u001b[0m output \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(output, object_hook \u001b[39m=\u001b[39m _utf_to_str)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/cpuinfo/cpuinfo.py:2742\u001b[0m, in \u001b[0;36mget_cpu_info_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2740\u001b[0m command \u001b[39m=\u001b[39m [sys\u001b[39m.\u001b[39mexecutable, \u001b[39m__file__\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m--json\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m   2741\u001b[0m p1 \u001b[39m=\u001b[39m Popen(command, stdout\u001b[39m=\u001b[39mPIPE, stderr\u001b[39m=\u001b[39mPIPE, stdin\u001b[39m=\u001b[39mPIPE)\n\u001b[0;32m-> 2742\u001b[0m output \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39;49mcommunicate()[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2744\u001b[0m \u001b[39mif\u001b[39;00m p1\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2745\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1134\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1979\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1973\u001b[0m                         stdout, stderr,\n\u001b[1;32m   1974\u001b[0m                         skip_check_and_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# Impossible :)\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1977\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfailed to raise TimeoutExpired.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1979\u001b[0m ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m   1980\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   1982\u001b[0m \u001b[39m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m \u001b[39m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1. Detection\n",
    "\n",
    "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
    "model.train(data='coco128.yaml', epochs=2)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.206 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.203 üöÄ Python-3.9.7 torch-2.1.0 CPU (Intel Core(TM) i5-5250U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=coco128.yaml, epochs=2, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train8\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n",
      "YOLOv8n-seg summary: 261 layers, 3409968 parameters, 3409952 gradients, 12.8 GFLOPs\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train8', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macbookair/Documents/Uni/PAAI/Code/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train8\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/8 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ERROR ‚ùå segment dataset incorrectly formatted or not a segment dataset.\nThis error can occur when incorrectly training a 'segment' model on a 'detect' dataset, i.e. 'yolo train model=yolov8n-seg.pt data=coco128.yaml'.\nVerify your dataset is a correctly formatted 'segment' dataset using 'data=coco128-seg.yaml' as an example.\nSee https://docs.ultralytics.com/tasks/segment/ for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/utils/loss.py:239\u001b[0m, in \u001b[0;36mv8SegmentationLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    238\u001b[0m batch_idx \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((batch_idx, batch[\u001b[39m'\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), batch[\u001b[39m'\u001b[39;49m\u001b[39mbboxes\u001b[39;49m\u001b[39m'\u001b[39;49m]), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    240\u001b[0m targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(targets\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), batch_size, scale_tensor\u001b[39m=\u001b[39mimgsz[[\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 157 but got size 0 for tensor number 1 in the list.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n-seg.pt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# load a pretrained YOLOv8n segmentation model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcoco128.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macbookair/Documents/Uni/PAAI/Code/model.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model(\u001b[39m'\u001b[39m\u001b[39mhttps://ultralytics.com/images/bus.jpg\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# predict on an image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/engine/model.py:341\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    340\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    342\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/engine/trainer.py:192\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/engine/trainer.py:343\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp):\n\u001b[1;32m    342\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m world_size\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/nn/tasks.py:41\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/nn/tasks.py:212\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_criterion()\n\u001b[1;32m    211\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mif\u001b[39;00m preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m preds\n\u001b[0;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(preds, batch)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ultralytics/utils/loss.py:244\u001b[0m, in \u001b[0;36mv8SegmentationLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    242\u001b[0m     mask_gt \u001b[39m=\u001b[39m gt_bboxes\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mgt_(\u001b[39m0\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mERROR ‚ùå segment dataset incorrectly formatted or not a segment dataset.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    245\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThis error can occur when incorrectly training a \u001b[39m\u001b[39m'\u001b[39m\u001b[39msegment\u001b[39m\u001b[39m'\u001b[39m\u001b[39m model on a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdetect\u001b[39m\u001b[39m'\u001b[39m\u001b[39m dataset, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mi.e. \u001b[39m\u001b[39m'\u001b[39m\u001b[39myolo train model=yolov8n-seg.pt data=coco128.yaml\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mVerify your dataset is a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mcorrectly formatted \u001b[39m\u001b[39m'\u001b[39m\u001b[39msegment\u001b[39m\u001b[39m'\u001b[39m\u001b[39m dataset using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata=coco128-seg.yaml\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mas an example.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSee https://docs.ultralytics.com/tasks/segment/ for help.\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# Pboxes\u001b[39;00m\n\u001b[1;32m    251\u001b[0m pred_bboxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[39m# xyxy, (b, h*w, 4)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ERROR ‚ùå segment dataset incorrectly formatted or not a segment dataset.\nThis error can occur when incorrectly training a 'segment' model on a 'detect' dataset, i.e. 'yolo train model=yolov8n-seg.pt data=coco128.yaml'.\nVerify your dataset is a correctly formatted 'segment' dataset using 'data=coco128-seg.yaml' as an example.\nSee https://docs.ultralytics.com/tasks/segment/ for help."
     ]
    }
   ],
   "source": [
    "#2. Segmentation\n",
    "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
    "model.train(data='coco128.yaml', epochs=2)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Classification\n",
    "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
    "model.train(data='mnist160', epochs=2)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Pose\n",
    "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "model.train(data='coco8-pose.yaml', epochs=2)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
